<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="孔祥旭">
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Scrapy 教程 - Scrapy 1.5 中文文档</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Scrapy \u6559\u7a0b";
    var mkdocs_page_input_path = "04-scrapy tutorial.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Scrapy 1.5 中文文档</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../01-index/">第一步</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../02-overview/">一眼了解Scrapy</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../03-install/">安装指导</a>
	    </li>
          
            <li class="toctree-l1 current">
		
    <a class="current" href="./">Scrapy 教程</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#scrapy">Scrapy 教程</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#_1">创建一个项目</a></li>
        
            <li><a class="toctree-l3" href="#spider">我们的第一个爬虫(Spider)</a></li>
        
            <li><a class="toctree-l3" href="#_2">如何运行我们的爬虫</a></li>
        
            <li><a class="toctree-l3" href="#_3">引擎盖下面发生了什么?</a></li>
        
            <li><a class="toctree-l3" href="#start_requests">一个简短的start_requests方法</a></li>
        
            <li><a class="toctree-l3" href="#_4">数据提取</a></li>
        
            <li><a class="toctree-l3" href="#xpath">XPATH 简短介绍</a></li>
        
            <li><a class="toctree-l3" href="#quotes">提取quotes和作者</a></li>
        
            <li><a class="toctree-l3" href="#_5">从我们的爬虫里提取数据</a></li>
        
            <li><a class="toctree-l3" href="#_6">存储抓取的数据</a></li>
        
            <li><a class="toctree-l3" href="#_7">跟踪连接</a></li>
        
            <li><a class="toctree-l3" href="#requests">创建一个简单的Requests, 并且用上追踪连接</a></li>
        
            <li><a class="toctree-l3" href="#_8">更多的例子和模式</a></li>
        
            <li><a class="toctree-l3" href="#spider_1">使用Spider参数</a></li>
        
            <li><a class="toctree-l3" href="#_9">下一步</a></li>
        
        </ul>
    

    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../05-examples/">例子</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../06-command_line_tool/">命令行工具</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../07-spiders/">Spiders 自定义爬虫类</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../08-selctor/">Selector 选择器</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../09-items/">Items</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../10-itemloaders/">Item Loaders(Item加载器)</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Scrapy 1.5 中文文档</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Scrapy 教程</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="scrapy">Scrapy 教程</h1>
<p>在本教程中，我们假设Scrapy已经安装在您的系统上。如果不是这样，请参阅<a href="#">安装指南</a>。</p>
<p>我们将抓取<a href="http://quotes.toscrape.com/">toscrape.com</a>，一个列出著名作家的引用的网站。</p>
<p>本教程将带领您完成以下任务:</p>
<ol>
<li>创建一个新的Scrapy项目</li>
<li>编写一个<a href="#">spider</a>来抓取站点并提取数据</li>
<li>使用命令行导出抓取的数据</li>
<li>将spider更改为递归跟踪链接</li>
<li>使用spider参数</li>
</ol>
<p>Scrapy是用<a href="https://www.python.org/">Python</a>编写的。如果你对这门语言不熟悉，你可能会想先了解一下这门语言是什么样子的，这样你就能最大限度地从困境中解脱出来。</p>
<p>如果您已经熟悉其他语言，并且希望快速学习Python，我们建议您<a href="http://www.diveintopython3.net/">通读Python 3</a>。或者，您可以遵循<a href="https://docs.python.org/3/tutorial">Python教程</a>。</p>
<p>如果您是一个编程新手，并且想从Python开始，您可能会发现在线书籍<a href="https://learnpythonthehardway.org/book/">《艰难地学习Python》</a>很有用。您还可以查看一下<a href="https://wiki.python.org/moin/BeginnersGuide/NonProgrammers">针对非程序员的Python资源列表</a>。</p>
<h2 id="_1">创建一个项目</h2>
<p>在你开始抓取之前，你必须建立一个新的Scrapy项目。输入想要存储代码的目录并运行:</p>
<p><code>scrapy startproject quotes</code></p>
<p>这将创建一个包含以下内容的目录:</p>
<pre><code>quotes/
    scrapy.cfg            # 部署配置文件

    quotes/             # 项目的Python模块，您将从这里导入代码
        __init__.py

        items.py          # 项目的items

        middlewares.py    # 项目的中间件

        pipelines.py      # 项目的pipeline

        settings.py       # 项目的settings

        spiders/          # 一个目录，稍后您将在其中放置您的spider(爬虫)
            __init__.py
</code></pre>

<h2 id="spider">我们的第一个爬虫(Spider)</h2>
<p>Spiders是您定义的类，Scrapy用于从网站(或一组网站)中提取信息。他们必须是<strong><a href="#">scrapy.Spider</a></strong>的子类并且定义初始requests。可以选择如何跟踪页面中的链接，以及如何解析下载的页面内容以提取数据。</p>
<p>这是第一个Spider(爬虫)的代码。将其保存在名为<code>quotes_spider.py</code>的文件中。然后放在项目名/spiders目录下:</p>
<pre><code>import scrapy


class QuotesSpider(scrapy.Spider):
    name = &quot;quotes&quot;

    def start_requests(self):
        urls = [
            'http://quotes.toscrape.com/page/1/',
            'http://quotes.toscrape.com/page/2/',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        page = response.url.split(&quot;/&quot;)[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
        self.log('Saved file %s' % filename)
</code></pre>

<p>如您所见，我们的Spider子类<strong><a href="#">scrapy.Spider</a></strong>。并定义了一些属性和方法:</p>
<ul>
<li><em><a href="#">name</a></em> : 标识了Spider。它必须在项目中是唯一的，也就是说，您不能为不同的Spider设置相同的名称。</li>
<li><em><a href="#">start_requests()</a></em> : 必须返回可以迭代的请求(您可以定义请求列表或编写列表推导式)，Spider将从该迭代开始抓取。后续请求将从这些初始请求中依次生成。</li>
<li><em><a href="#">parse()</a></em> : 被调用处理每个请求解析完之后后续处理的方法。response参数是<em><a href="#">TextResponse</a></em>的一个实例，它保存了页面内容，并有进一步有用的方法来处理它。</li>
</ul>
<p>parse()方法通常解析response，将抓取的数据提取为字典，并找到要跟踪的新url，并从中创建新的请求(<a href="#">Request</a>)。</p>
<h2 id="_2">如何运行我们的爬虫</h2>
<p>要让spider正常工作，请转到项目的跟目录并运行:</p>
<pre><code>scrapy crawl quotes
</code></pre>

<p>这个命令用我们刚刚添加的Spider名字<code>quotes</code>，它将发送一些对<code>http://quotes.toscrape.com/</code>域的请求。您将得到一个类似的输出:</p>
<pre><code>... (omitted for brevity)
2016-12-16 21:24:05 [scrapy.core.engine] INFO: Spider opened
2016-12-16 21:24:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-12-16 21:24:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (404) &lt;GET http://quotes.toscrape.com/robots.txt&gt; (referer: None)
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://quotes.toscrape.com/page/1/&gt; (referer: None)
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://quotes.toscrape.com/page/2/&gt; (referer: None)
2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-1.html
2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-2.html
2016-12-16 21:24:05 [scrapy.core.engine] INFO: Closing spider (finished)
...
</code></pre>

<p>现在，检查当前目录中的文件。您应该注意到已经创建了两个新文件:quotes-1.html和quotes-2.html，以及相应url的内容，如我们的<code>parse</code>方法所示。</p>
<pre><code>注意!!!
如果您想知道为什么我们还没有解析HTML，请稍等，我们很快就会讲到。
</code></pre>

<h2 id="_3">引擎盖下面发生了什么?</h2>
<p>Scrapy调度由Spider的<code>start_requests</code>方法返回的<em><a href="#">scrapy.Request</a></em>对象。在接收到每个response之后，它实例化<em><a href="#">Response</a></em>对象并调用与request关联的回调方法(在本例中是<code>parse</code>方法)，将response作为参数传递。</p>
<h2 id="start_requests">一个简短的start_requests方法</h2>
<pre><code>import scrapy


class QuotesSpider(scrapy.Spider):
    name = &quot;quotes&quot;
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
        'http://quotes.toscrape.com/page/2/',
    ]

    def parse(self, response):
        page = response.url.split(&quot;/&quot;)[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
</code></pre>

<p><em><a href="#">parse()</a></em>方法被调用来处理这些url的每个requests，尽管我们还没有明确告诉Scrapy这样做。这是因为<em><a href="#">parse()</a></em>是Scrapy的默认回调方法，所以在没有显式分配回调的情况下调用requests</p>
<h2 id="_4">数据提取</h2>
<p>学习如何使用Scrapy最佳提取数据方法是使用<a href="#">Scrapy shell</a>尝试选择器。运行:</p>
<pre><code>scrapy shell 'http://quotes.toscrape.com/page/1/'
</code></pre>

<pre><code>注意!!!!
记住，在命令行中运行Scrapy shell时，必须用引号将url括起来，否则url包含参数(比如&amp;字符)不会工作。
在windows上请使用双引号
</code></pre>

<p>你会看到这样的东西:</p>
<pre><code>[ ... Scrapy log here ... ]
2016-09-19 12:09:27 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://quotes.toscrape.com/page/1/&gt; (referer: None)
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x7fa91d888c90&gt;
[s]   item       {}
[s]   request    &lt;GET http://quotes.toscrape.com/page/1/&gt;
[s]   response   &lt;200 http://quotes.toscrape.com/page/1/&gt;
[s]   settings   &lt;scrapy.settings.Settings object at 0x7fa91d888c10&gt;
[s]   spider     &lt;DefaultSpider 'default' at 0x7fa91c8af990&gt;
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser
&gt;&gt;&gt;
</code></pre>

<p>使用shell，您可以尝试使用<a href="#">CSS</a>和response对象选择元素:</p>
<pre><code>&gt;&gt;&gt; response.css('title')
[&lt;Selector xpath='descendant-or-self::title' data='&lt;title&gt;Quotes to Scrape&lt;/title&gt;'&gt;]
</code></pre>

<p>运行<code>response.css('title')</code>的结果是一个类似于列表的<a href="#">SelectorList</a>对象，它表示一组<a href="#">Selector</a>对象，这些<a href="#">Selector</a>对象围绕XML/HTML元素，允许您运行进一步的查询来细化选择或提取数据。</p>
<p>要从上面的标题中提取文本，您可以这样做:</p>
<pre><code>&gt;&gt;&gt; response.css('title::text').extract()
['Quotes to Scrape']
</code></pre>

<p>这里有两件事需要注意:一是我们已经在CSS查询中添加了<code>::text</code>，意思是我们只想直接选择里面的<code>&lt;title&gt;</code>文本元素, 如果我们不指定<code>::text</code>, 我们会得到完整的标题元素，包括它的标签:</p>
<pre><code>&gt;&gt;&gt; response.css('title').extract()
['&lt;title&gt;Quotes to Scrape&lt;/title&gt;']
</code></pre>

<p>另一件事是调用<code>.extract()</code>的结果是一个列表，因为我们正在处理<em><a href="#">SelectorList</a></em>的一个实例。当你知道你只是想要第一个结果的时候，在这种情况下，你可以这样做:</p>
<pre><code>&gt;&gt;&gt; response.css('title::text').extract_first()
'Quotes to Scrape'
</code></pre>

<p>作为替代，你可以这样写:</p>
<pre><code>&gt;&gt;&gt; response.css('title::text')[0].extract()
'Quotes to Scrape'
</code></pre>

<p>但是，使用<code>.extract_first()</code>可以避免出现<code>IndexError</code>，并且当它没有找到任何匹配选择的元素时，返回<code>None</code>。</p>
<p>这里有一个教训:对于大多数抓取代码，您希望它能够对由于在页面上找不到东西而导致的错误保持容错, 这样即使某些部分不能被抓取，您至少可以获得<strong>一些</strong>数据。</p>
<p>除了<code>extract()</code>和<code>extract_first()</code>方法之外，您还可以使用正则表达式<code>re()</code>方法进行提取:</p>
<pre><code>&gt;&gt;&gt; response.css('title::text').re(r'Quotes.*')
['Quotes to Scrape']
&gt;&gt;&gt; response.css('title::text').re(r'Q\w+')
['Quotes']
&gt;&gt;&gt; response.css('title::text').re(r'(\w+) to (\w+)')
['Quotes', 'Scrape']
</code></pre>

<p>为了找到合适的CSS选择器，您可能会发现使用<code>view(response)</code>从web浏览器的shell中打开response页面非常有用。您可以使用浏览器开发工具或扩展，比如Firebug(请参阅使用<a href="#">Firebug进行抓取</a>和<a href="#">使用Firefox进行抓取</a>部分)。</p>
<p><a href="https://selectorgadget.com/">Selector Gadget</a>也是一个很好的工具，可以通过可视化的CSS选择器快速找到元素，这在许多浏览器中都可以工作。</p>
<h2 id="xpath">XPATH 简短介绍</h2>
<p>除了<a href="https://www.w3.org/TR/selectors">CSS</a>,Scrapy selectors还支持使用<a href="https://www.w3.org/TR/xpath">XPath</a>表达式:</p>
<pre><code>&gt;&gt;&gt; response.xpath('//title')
[&lt;Selector xpath='//title' data='&lt;title&gt;Quotes to Scrape&lt;/title&gt;'&gt;]
&gt;&gt;&gt; response.xpath('//title/text()').extract_first()
'Quotes to Scrape'
</code></pre>

<p>XPath表达式非常强大，是Scrapy Selectors的基础。实际上，CSS选择器在底层被转换为XPath。如果仔细阅读shell中的选择器对象的文本表示，就会发现这一点。</p>
<p>尽管不如CSS选择器流行，XPath表达式提供了更多的功能，因为除了导航结构外，它还可以查看内容。使用XPath，您可以选择如下内容:选择包含文本“下一页”的链接。这使得XPath非常适合于抓取任务，我们鼓励您学习XPath，即使您已经知道如何构造CSS选择器，它也会使抓取变得更加容易。</p>
<p>我们在这里不会介绍太多XPath，但是您可以在这里阅读关于使用<a href="#">XPath和Scrapy选择器</a>的更多信息。要了解更多关于XPath的知识，我们推荐<a href="http://zvon.org/comp/r/tut-XPath_1.html">本教程通过示例学习XPath</a>，<a href="http://plasmasturm.org/log/xpath101/">本教程学习“如何在XPath中思考”</a>。</p>
<h2 id="quotes">提取quotes和作者</h2>
<p>现在您已经了解了一些关于选择和提取的知识，让我们通过编写从web页面中提取quotes的代码来完成爬虫。</p>
<p><a href="http://quotes.toscrape.com">http://quotes.toscrape.com</a>中的每个引语都由HTML元素表示，如下所示:</p>
<pre><code>&lt;div class=&quot;quote&quot;&gt;
    &lt;span class=&quot;text&quot;&gt;“The world as we have created it is a process of our
    thinking. It cannot be changed without changing our thinking.”&lt;/span&gt;
    &lt;span&gt;
        by &lt;small class=&quot;author&quot;&gt;Albert Einstein&lt;/small&gt;
        &lt;a href=&quot;/author/Albert-Einstein&quot;&gt;(about)&lt;/a&gt;
    &lt;/span&gt;
    &lt;div class=&quot;tags&quot;&gt;
        Tags:
        &lt;a class=&quot;tag&quot; href=&quot;/tag/change/page/1/&quot;&gt;change&lt;/a&gt;
        &lt;a class=&quot;tag&quot; href=&quot;/tag/deep-thoughts/page/1/&quot;&gt;deep-thoughts&lt;/a&gt;
        &lt;a class=&quot;tag&quot; href=&quot;/tag/thinking/page/1/&quot;&gt;thinking&lt;/a&gt;
        &lt;a class=&quot;tag&quot; href=&quot;/tag/world/page/1/&quot;&gt;world&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;
</code></pre>

<p>让我们打开scrapy shell并测试这个网站，以了解如何提取我们想要的数据:</p>
<pre><code>$ scrapy shell 'http://quotes.toscrape.com'
</code></pre>

<p>我们得到了一个引用HTML元素的选择器列表:</p>
<pre><code>&gt;&gt;&gt; response.css(&quot;div.quote&quot;)
</code></pre>

<p>上面查询返回的每个选择器都允许我们对它们的子元素运行进一步的查询。让我们把第一个选择器分配给一个变量，这样我们就可以直接在一个引用上运行我们的CSS选择器:</p>
<pre><code>&gt;&gt;&gt; quote = response.css(&quot;div.quote&quot;)[0]
</code></pre>

<p>现在，让我们使用刚才创建的quote对象从引用中提取标题、作者和标签:</p>
<pre><code>&gt;&gt;&gt; title = quote.css(&quot;span.text::text&quot;).extract_first()
&gt;&gt;&gt; title
'“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'
&gt;&gt;&gt; author = quote.css(&quot;small.author::text&quot;).extract_first()
&gt;&gt;&gt; author
'Albert Einstein'
</code></pre>

<p>假设标记是字符串列表，我们可以使用<code>.extract()</code>方法获得所有字符串:</p>
<pre><code>&gt;&gt;&gt; tags = quote.css(&quot;div.tags a.tag::text&quot;).extract()
&gt;&gt;&gt; tags
['change', 'deep-thoughts', 'thinking', 'world']
</code></pre>

<p>知道了如何提取每一位的数据，我们现在可以遍历所有的quote元素，并将它们放到Python字典中:</p>
<pre><code>&gt;&gt;&gt; for quote in response.css(&quot;div.quote&quot;):
...     text = quote.css(&quot;span.text::text&quot;).extract_first()
...     author = quote.css(&quot;small.author::text&quot;).extract_first()
...     tags = quote.css(&quot;div.tags a.tag::text&quot;).extract()
...     print(dict(text=text, author=author, tags=tags))
{'tags': ['change', 'deep-thoughts', 'thinking', 'world'], 'author': 'Albert Einstein', 'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'}
{'tags': ['abilities', 'choices'], 'author': 'J.K. Rowling', 'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”'}
    ... a few more of these, omitted for brevity
&gt;&gt;&gt;
</code></pre>

<h2 id="_5">从我们的爬虫里提取数据</h2>
<p>让我们回到我们的spider。到目前为止，它并没有提取任何数据，只是将整个HTML页面保存到本地文件中。让我们将上面的提取逻辑集成到我们的spider中。</p>
<p>Scrapy spider通常生成许多包含从页面提取的数据的字典。为此，我们在回调中使用<code>yield</code> Python关键字，如下所示:</p>
<pre><code>import scrapy


class QuotesSpider(scrapy.Spider):
    name = &quot;quotes&quot;
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
        'http://quotes.toscrape.com/page/2/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').extract_first(),
                'author': quote.css('small.author::text').extract_first(),
                'tags': quote.css('div.tags a.tag::text').extract(),
            }
</code></pre>

<p>如果您运行这个爬虫，它将输出带日志的提取数据:</p>
<pre><code>2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://quotes.toscrape.com/page/1/&gt;
{'tags': ['life', 'love'], 'author': 'André Gide', 'text': '“It is better to be hated for what you are than to be loved for what you are not.”'}
2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://quotes.toscrape.com/page/1/&gt;
{'tags': ['edison', 'failure', 'inspirational', 'paraphrased'], 'author': 'Thomas A. Edison', 'text': &quot;“I have not failed. I've just found 10,000 ways that won't work.”&quot;}
</code></pre>

<h2 id="_6">存储抓取的数据</h2>
<p>存储抓取的数据的最简单方法是使用<a href="#">Feed exports</a>，使用以下命令:</p>
<pre><code>scrapy crawl quotes -o quotes.json
</code></pre>

<p>这会产生一个quotes.json文件，包含所有抓取过的items并<a href="https://en.wikipedia.org/wiki/JSON">json</a>序列化。</p>
<p>出于历史原因，Scrapy将追加一个给定的文件而不是覆盖。如果您在第二次运行该命令两次而没有在第二次之前删除该文件，那么最终会得到一个损坏的JSON文件。</p>
<p>还可以使用其他格式，比如<a href="http://jsonlines.org/">JSON行</a>:</p>
<pre><code>scrapy crawl quotes -o quotes.jl
</code></pre>

<p>JSON行格式很有用，因为它类似于流，您可以很容易地向其追加新记录。当你运行两次时，它不会有JSON那样的问题。另外，由于每个记录都是单独的一行，您可以处理大文件，而不必将所有内容都放在内存中。</p>
<p>在小项目中(如本教程中的项目)，这就足够了。但是，如果您想要对抓取的数据执行更复杂的操作，您可以编写一个<a href="#">Item Pipeline</a>。在<code>tutorial/pipelines.py</code> 中，项目创建时为您设置了Item Pipeline的占位符文件。不过，如果您只是想要存储抓取的item，则不需要实例化任何item pipelines。</p>
<h2 id="_7">跟踪连接</h2>
<p>比方说，不要只抓取<a href="http://quotes.toscrape.com">http://quotes.toscrape.com</a>前两页，想抓取所有页面的引文。</p>
<p>既然您已经知道了如何从页面中提取数据，那么让我们看看如何从页面中跟踪链接。
第一件事是提取到我们想要跟随的页面的链接。检查我们的页面，我们可以看到有一个链接到下一页的标签:</p>
<pre><code>&lt;ul class=&quot;pager&quot;&gt;
    &lt;li class=&quot;next&quot;&gt;
        &lt;a href=&quot;/page/2/&quot;&gt;Next &lt;span aria-hidden=&quot;true&quot;&gt;&amp;rarr;&lt;/span&gt;&lt;/a&gt;
    &lt;/li&gt;
&lt;/ul&gt;
</code></pre>

<p>我们可以试着从shell中提取出来:</p>
<pre><code>&gt;&gt;&gt; response.css('li.next a').extract_first()
'&lt;a href=&quot;/page/2/&quot;&gt;Next &lt;span aria-hidden=&quot;true&quot;&gt;→&lt;/span&gt;&lt;/a&gt;'
</code></pre>

<p>这将获取超链接元素，但我们需要<code>href</code>属性。为此，Scrapy支持一个CSS扩展，允许您选择属性内容，如下所示:</p>
<pre><code>&gt;&gt;&gt; response.css('li.next a::attr(href)').extract_first()
'/page/2/'
</code></pre>

<p>现在让我们看看我们的爬虫被修改成递归地跟随链接到下一页，然后从中提取数据:</p>
<pre><code>import scrapy


class QuotesSpider(scrapy.Spider):
    name = &quot;quotes&quot;
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').extract_first(),
                'author': quote.css('small.author::text').extract_first(),
                'tags': quote.css('div.tags a.tag::text').extract(),
            }

        next_page = response.css('li.next a::attr(href)').extract_first()
        if next_page is not None:
            next_page = response.urljoin(next_page)
            yield scrapy.Request(next_page, callback=self.parse)
</code></pre>

<p>现在,经过提取数据,<code>parse()</code>方法寻找到下一个页面的链接,使用<a href="#">urljoin()</a>方法构建完整的绝对URL(因为链接可以是相对的),并向下一页生成一个新请求，设置其回调函数，以处理下一页的数据提取并保持所有页面的爬行。</p>
<p>您在这里看到的是Scrapy的链接机制:当您在回调方法中生成请求时，Scrapy将调度发送该请求，并设置一个回调方法以便在该请求完成时执行。</p>
<p>使用此功能，您可以构建复杂的爬虫程序，根据您定义的规则跟踪链接，并根据访问的页面提取不同类型的数据。</p>
<p>在我们的示例中，它创建了一种循环，跟踪到下一个页面的所有链接，直到找不到为止, 这对于在带有分页的博客、论坛和其他站点上抓取非常方便。</p>
<h2 id="requests">创建一个简单的Requests, 并且用上追踪连接</h2>
<p>您可以使用<a href="#">response.follow</a>:</p>
<pre><code>import scrapy


class QuotesSpider(scrapy.Spider):
    name = &quot;quotes&quot;
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').extract_first(),
                'author': quote.css('span small::text').extract_first(),
                'tags': quote.css('div.tags a.tag::text').extract(),
            }

        next_page = response.css('li.next a::attr(href)').extract_first()
        if next_page is not None:
            yield response.follow(next_page, callback=self.parse)
</code></pre>

<p>不像scrapy.Request, <code>response.follow</code>支持url相对路径 -不需要调用urljoin。 注意, <code>response.follow</code>只返回一个Request实例;您仍然必须yield此请求。</p>
<p>您还可以传递一个选择器给<code>response.follow</code>。 而不是字符串;这个选择器应该提取必要的属性(<strong>PS:个人理解必须是链接</strong>):</p>
<pre><code>for href in response.css('li.next a::attr(href)'):
    yield response.follow(href, callback=self.parse)
</code></pre>

<p>对于<code>&lt;a&gt;</code>元素，有一个简单方式: <code>response.follow</code>会自动使用它们的href属性。因此，代码可以进一步缩短:</p>
<pre><code>for a in response.css('li.next a'):
    yield response.follow(a, callback=self.parse)
</code></pre>

<pre><code>注意!!!!
response.follow(response.css('li.next a')))是无效的，
因为response.css返回一个类似于列表的对象，
其中包含所有结果的选择器，而不是单个选择器。
正确的方式比如一个for循环就像上面的例子，或者response.follow(response.css('li.next a')[0])就很好。
</code></pre>

<h2 id="_8">更多的例子和模式</h2>
<p>下面是另一个爬虫，它演示了回调和追踪链接，这次是为了抓取作者信息:</p>
<pre><code>import scrapy


class AuthorSpider(scrapy.Spider):
    name = 'author'

    start_urls = ['http://quotes.toscrape.com/']

    def parse(self, response):
        # follow links to author pages
        for href in response.css('.author + a::attr(href)'):
            yield response.follow(href, self.parse_author)

        # follow pagination links
        for href in response.css('li.next a::attr(href)'):
            yield response.follow(href, self.parse)

    def parse_author(self, response):
        def extract_with_css(query):
            return response.css(query).extract_first().strip()

        yield {
            'name': extract_with_css('h3.author-title::text'),
            'birthdate': extract_with_css('.author-born-date::text'),
            'bio': extract_with_css('.author-description::text'),
        }
</code></pre>

<p>这个爬虫将从主页开始，它追踪当前页面的信息回调给<code>parse_author</code>, 并且解析下一页的链接回调给<code>parse</code></p>
<p>这里我们传递回调用<code>response.follow</code>作为位置参数使代码更短;它也适用于<code>scrapy.Request</code>。</p>
<p><code>parse_author</code>回调函数定义了一个有用的函数，用于从CSS查询中提取和清理数据，并生成带有作者数据的Python字典。</p>
<p>这个爬虫演示的另一件有趣的事情是，即使来自同一作者的引用很多次，我们也不必担心多次访问同一作者的页面。默认情况下，Scrapy会过滤掉已经访问过的url的重复请求，从而避免由于编程错误而过多地攻击服务器的问题。这可以通过设置<a href="#">DUPEFILTER_CLASS</a>来配置。</p>
<p>希望现在您已经很好地理解了如何使用Scrapy跟踪链接和回调机制。</p>
<p>作为利用跟踪链接机制的另一个示例，请查看<a href="#">CrawlSpider</a>类，它是一个通用的spider，它实现了一个小规则引擎，您可以使用它在上面编写爬虫程序。</p>
<p>另外，一种常见的模式是使用多个页面的数据构建一个项目，使用<a href="#">向回调函数传递额外数据的技巧</a>。</p>
<h2 id="spider_1">使用Spider参数</h2>
<p>在运行爬虫时，可以使用-a选项为爬虫提供命令行参数:</p>
<pre><code>scrapy crawl quotes -o quotes-humor.json -a tag=humor
</code></pre>

<p>这些参数被传递给Spider的<code>__init__</code>方法，并默认成为spider属性。</p>
<p>在本例中，传递的<code>tag</code>的值可以通过<code>self.tag</code>属性引用。你可以用它来让你的爬行器抓取只带有特定标签的quotes，根据参数建立URL:</p>
<pre><code>import scrapy


class QuotesSpider(scrapy.Spider):
    name = &quot;quotes&quot;

    def start_requests(self):
        url = 'http://quotes.toscrape.com/'
        tag = getattr(self, 'tag', None)
        if tag is not None:
            url = url + 'tag/' + tag
        yield scrapy.Request(url, self.parse)

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').extract_first(),
                'author': quote.css('small.author::text').extract_first(),
            }

        next_page = response.css('li.next a::attr(href)').extract_first()
        if next_page is not None:
            yield response.follow(next_page, self.parse)
</code></pre>

<p>如果您将<code>tag=humor</code>参数传递给这个spider，您会注意到它只会访问来自<code>humor</code>标记的url，比如<code>http://quotes.toscrape.com/tag/humor</code>。</p>
<p>您可以<a href="#">在这里学习更多关于处理爬行器参数的知识</a>。</p>
<h2 id="_9">下一步</h2>
<p>本教程只介绍了Scrapy的基础知识，但是这里没有提到其他许多特性。<a href="#">还有其他的么?</a>回顾一下<a href="https://qq351469076.github.io/Scrapy1.5.1-zh_CN-docs/1-index/">第一步</a>，可以快速浏览最重要的章节。</p>
<p>您可以继续从基本概念部分了解更多关于命令行工具、spiders、选择器和其他本教程没有涉及到的内容，比如对抓取的数据进行建模。如果您喜欢使用示例项目，请检查示例部分。</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../05-examples/" class="btn btn-neutral float-right" title="例子">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../03-install/" class="btn btn-neutral" title="安装指导"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../03-install/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../05-examples/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
