<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="孔祥旭 qq:351469076 微信:kxx351469076">
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Item Pipeline - Scrapy 1.5.1 中文文档</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Item Pipeline";
    var mkdocs_page_input_path = "2. \u57fa\u672c\u6982\u5ff5\\12-item_pipeline.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Scrapy 1.5.1 中文文档</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">1. 第一步</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../1. 第一步/01-index/">Scrapy 1.5 中文文档</a>
                </li>
                <li class="">
                    
    <a class="" href="../../1. 第一步/02-overview/">一眼了解Scrapy</a>
                </li>
                <li class="">
                    
    <a class="" href="../../1. 第一步/03-install/">安装指导</a>
                </li>
                <li class="">
                    
    <a class="" href="../../1. 第一步/04-scrapy tutorial/">Scrapy 教程</a>
                </li>
                <li class="">
                    
    <a class="" href="../../1. 第一步/05-examples/">例子</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">2. 基本概念</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../06-command_line_tool/">命令行工具</a>
                </li>
                <li class="">
                    
    <a class="" href="../07-spiders/">Spiders 自定义爬虫类</a>
                </li>
                <li class="">
                    
    <a class="" href="../08-selctor/">Selector 选择器</a>
                </li>
                <li class="">
                    
    <a class="" href="../09-items/">Items</a>
                </li>
                <li class="">
                    
    <a class="" href="../10-itemloaders/">Item Loaders(Item加载器)</a>
                </li>
                <li class="">
                    
    <a class="" href="../11-scrapy_shell/">Scrapy shell</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Item Pipeline</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#item-pipeline">Item Pipeline</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#item-pipeline_1">编写自己的Item Pipeline</a></li>
        
            <li><a class="toctree-l4" href="#item-pipeline_2">Item pipeline的例子</a></li>
        
            <li><a class="toctree-l4" href="#items">价格验证和没有价格就删掉items</a></li>
        
            <li><a class="toctree-l4" href="#itemsjson">将items写入JSON文件</a></li>
        
            <li><a class="toctree-l4" href="#mongodbitems">向MongoDB写入items</a></li>
        
            <li><a class="toctree-l4" href="#item">item截图</a></li>
        
            <li><a class="toctree-l4" href="#duplicates-filter">Duplicates filter(重复过滤器)</a></li>
        
            <li><a class="toctree-l4" href="#item-pipeline_3">激活Item pipeline组件</a></li>
        
        </ul>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../13-feed_exports/">Feed exports</a>
                </li>
                <li class="">
                    
    <a class="" href="../about/">about</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Scrapy 1.5.1 中文文档</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>2. 基本概念 &raquo;</li>
        
      
    
    <li>Item Pipeline</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="item-pipeline">Item Pipeline</h1>
<p>在一个item被一个spider抓取后，它被发送到Item Pipeline，该管道通过顺序执行的几个组件处理它。</p>
<p>每个item pipeline组件(有时称为“项目管道”)都是实现简单方法的Python类。他们接收一个item并对其执行一个操作，同时还决定该item是否应该通过管道继续进行，或者被删除，不再进行处理。</p>
<p>Item Pipeline的典型用途有:</p>
<ul>
<li>清理HTML数据</li>
<li>验证抓取的数据(检查items是否包含某些字段)</li>
<li>检查副本(并删除它们)</li>
<li>将抓取的数据存储在数据库中</li>
</ul>
<h2 id="item-pipeline_1">编写自己的Item Pipeline</h2>
<p>每个Item Pipeline组件都是Python类，必须实现以下方法:</p>
<p><code>process_item(self, item, spider)</code></p>
<blockquote>
<p>对于每个item pipeline组件调用此方法。<a href="#">process_item()</a>必须:返回一个带有数据的字典，返回一个<a href="#">Item</a>(或任何子类)对象，返回一个<a href="https://twistedmatrix.com/documents/current/core/howto/defer.html">Twisted Deferred</a>或抛出<a href="#">DropItem</a>异常。被丢弃的items不再由其他pipeline组件处理。</p>
<p>参数:</p>
<blockquote>
<p>item (<a href="#">Item</a>对象或一个字典) - 抓取的item</p>
<p>spider (<a href="#">Spider</a>对象) - 哪个spider抓取的item</p>
</blockquote>
</blockquote>
<p>此外，它们还可以执行下列方法:</p>
<p><code>open_spider(self, spider)</code></p>
<blockquote>
<p>当启动spider时调用此方法。</p>
<p>参数:</p>
<blockquote>
<p>spider (Spider对象) – spider启动的时候</p>
</blockquote>
</blockquote>
<p><code>close_spider(self, spider)</code></p>
<blockquote>
<p>在关闭spider时调用此方法。</p>
<p>参数:</p>
<blockquote>
<p>spider (Spider对象) – spider关闭的时候</p>
</blockquote>
</blockquote>
<p><code>from_crawler(cls, crawler)</code></p>
<blockquote>
<p>如果存在，则调用这个类方法是为了从<a href="#">Crawler</a>中创建一个管道实例。它必须返回pipeline的新实例。Crawler对象提供访问所有Scrapy的核心组件，如settings(设置)和signals(信号);这是一种pipeline访问它们并将其功能连接到Scrapy的方法。</p>
<p>参数:</p>
<blockquote>
<p>crawler (Crawler对象) – 使用这个pipeline的crawler</p>
</blockquote>
</blockquote>
<h2 id="item-pipeline_2">Item pipeline的例子</h2>
<h2 id="items">价格验证和没有价格就删掉items</h2>
<p>让我们来看看下面这个假想的管道，它调整不包含VAT (<code>price_exclude des_vat</code>属性)的items的<code>price</code>属性，并删除不包含price的条目:</p>
<pre><code class="python">from scrapy.exceptions import DropItem

class PricePipeline(object):

    vat_factor = 1.15

    def process_item(self, item, spider):
        if item['price']:
            if item['price_excludes_vat']:
                item['price'] = item['price'] * self.vat_factor
            return item
        else:
            raise DropItem(&quot;Missing price in %s&quot; % item)
</code></pre>

<h2 id="itemsjson">将items写入JSON文件</h2>
<p>下面的管道将所有spiders中抓取所有的items存储为单个<code>items.jl</code>文件，每行包含一个item，以JSON格式序列化:</p>
<pre><code class="python">import json

class JsonWriterPipeline(object):

    def open_spider(self, spider):
        self.file = open('items.jl', 'w')

    def close_spider(self, spider):
        self.file.close()

    def process_item(self, item, spider):
        line = json.dumps(dict(item)) + &quot;\n&quot;
        self.file.write(line)
        return item
</code></pre>

<pre><code class="note">JsonWriterPipeline的目的是介绍如何编写Item pipeline。
如果您真的想将所有抓取的items到JSON文件中，那么应该使用Feed exports。
</code></pre>

<h2 id="mongodbitems">向MongoDB写入items</h2>
<p>在本例中，我们将使用<a href="https://api.mongodb.org/python/current/">pymongo</a>将items写入<a href="https://www.mongodb.org/">MongoDB</a>。MongoDB地址和数据库名在Scrapy settings中指定;MongoDB集合是以item类命名的。</p>
<p>这个例子的重点是展示如何使用<a href="#">from_crawler()</a>方法以及如何正确清理资源。</p>
<pre><code class="python">import pymongo

class MongoPipeline(object):

    collection_name = 'scrapy_items'

    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get('MONGO_URI'),
            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')
        )

    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]

    def close_spider(self, spider):
        self.client.close()

    def process_item(self, item, spider):
        self.db[self.collection_name].insert_one(dict(item))
        return item
</code></pre>

<h2 id="item">item截图</h2>
<p>这个例子演示了如何从<a href="#">process_item()</a>方法里<a href="https://twistedmatrix.com/documents/current/core/howto/defer.html">Deferred(延迟)</a>返回。它使用<a href="https://splash.readthedocs.io/en/stable/">Splash</a>来render item url的屏幕截图。Pipeline请求本地运行<a href="https://splash.readthedocs.io/en/stable/">Splash</a>实例。在下载请求并延迟回调触发后，它将item保存到文件中，并将文件名添加到item中。</p>
<pre><code class="python">import scrapy
import hashlib
from urllib.parse import quote


class ScreenshotPipeline(object):
    &quot;&quot;&quot;使用Splash来render每个Scrapy item的屏幕截图.&quot;&quot;&quot;

    SPLASH_URL = &quot;http://localhost:8050/render.png?url={}&quot;

    def process_item(self, item, spider):
        encoded_item_url = quote(item[&quot;url&quot;])
        screenshot_url = self.SPLASH_URL.format(encoded_item_url)
        request = scrapy.Request(screenshot_url)
        dfd = spider.crawler.engine.download(request, spider)
        dfd.addBoth(self.return_item, item)
        return dfd

    def return_item(self, response, item):
        if response.status != 200:
            # 发生错误，返回item.
            return item

        # 保存截图到文件，文件名将是url的哈希.
        url = item[&quot;url&quot;]
        url_hash = hashlib.md5(url.encode(&quot;utf8&quot;)).hexdigest()
        filename = &quot;{}.png&quot;.format(url_hash)
        with open(filename, &quot;wb&quot;) as f:
            f.write(response.body)

        # 在item中存储文件名.
        item[&quot;screenshot_filename&quot;] = filename
        return item
</code></pre>

<h2 id="duplicates-filter">Duplicates filter(重复过滤器)</h2>
<p>一个查找重复items的过滤器，并删除已处理的items。假设我们的item有一个唯一的id，但是spider返回多个具有相同id的items:</p>
<pre><code class="python">from scrapy.exceptions import DropItem

class DuplicatesPipeline(object):

    def __init__(self):
        self.ids_seen = set()

    def process_item(self, item, spider):
        if item['id'] in self.ids_seen:
            raise DropItem(&quot;Duplicate item found: %s&quot; % item)
        else:
            self.ids_seen.add(item['id'])
            return item
</code></pre>

<h2 id="item-pipeline_3">激活Item pipeline组件</h2>
<p>要激活Item pipeline组件，必须将其类添加到<a href="#">item_pipeline</a> setting中，如下例所示:</p>
<pre><code class="python">ITEM_PIPELINES = {
    'myproject.pipelines.PricePipeline': 300,
    'myproject.pipelines.JsonWriterPipeline': 800,
}
</code></pre>

<p>在这个setting中，您为类分配的整数值决定了它们运行的顺序:items从低值类到高值类。通常在0-1000范围内定义这些数字。</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../13-feed_exports/" class="btn btn-neutral float-right" title="Feed exports">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../11-scrapy_shell/" class="btn btn-neutral" title="Scrapy shell"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../11-scrapy_shell/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../13-feed_exports/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>

</body>
</html>
