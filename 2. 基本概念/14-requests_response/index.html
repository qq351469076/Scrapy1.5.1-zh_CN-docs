<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="孔祥旭 qq:351469076 微信:kxx351469076">
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Requests和Responses - Scrapy 1.5.1 中文文档</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Requests\u548cResponses";
    var mkdocs_page_input_path = "2. \u57fa\u672c\u6982\u5ff5\\14-requests_response.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Scrapy 1.5.1 中文文档</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">1. 第一步</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../1. 第一步/01-index/">Scrapy 1.5 中文文档</a>
                </li>
                <li class="">
                    
    <a class="" href="../../1. 第一步/02-overview/">一眼了解Scrapy</a>
                </li>
                <li class="">
                    
    <a class="" href="../../1. 第一步/03-install/">安装指导</a>
                </li>
                <li class="">
                    
    <a class="" href="../../1. 第一步/04-scrapy tutorial/">Scrapy 教程</a>
                </li>
                <li class="">
                    
    <a class="" href="../../1. 第一步/05-examples/">例子</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">2. 基本概念</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../06-command_line_tool/">Command line tool(命令行工具)</a>
                </li>
                <li class="">
                    
    <a class="" href="../07-spiders/">Spiders(自定义爬虫类)</a>
                </li>
                <li class="">
                    
    <a class="" href="../08-selctor/">Selector(选择器)</a>
                </li>
                <li class="">
                    
    <a class="" href="../09-items/">Items(模型)</a>
                </li>
                <li class="">
                    
    <a class="" href="../10-itemloaders/">Item Loaders(Item加载器)</a>
                </li>
                <li class="">
                    
    <a class="" href="../11-scrapy_shell/">Scrapy shell(Scrapy终端)</a>
                </li>
                <li class="">
                    
    <a class="" href="../12-item_pipeline/">Item Pipeline(模组管道)</a>
                </li>
                <li class="">
                    
    <a class="" href="../13-feed_exports/">Feed exports(导出各种格式文件)</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Requests和Responses</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#requestsresponses">Requests和Responses</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#request">Request对象</a></li>
        
            <li><a class="toctree-l4" href="#callback">将额外数据传递给callback函数</a></li>
        
            <li><a class="toctree-l4" href="#errback">使用errback在请求处理中捕获异常</a></li>
        
            <li><a class="toctree-l4" href="#requestmeta">Request.meta 特殊键</a></li>
        
            <li><a class="toctree-l4" href="#bindaddress">bindaddress</a></li>
        
            <li><a class="toctree-l4" href="#download_timeout">download_timeout</a></li>
        
            <li><a class="toctree-l4" href="#download_latency">download_latency</a></li>
        
            <li><a class="toctree-l4" href="#download_fail_on_dataloss">download_fail_on_dataloss</a></li>
        
            <li><a class="toctree-l4" href="#max_retry_times">max_retry_times</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#request-subclasses">Request subclasses</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#formrequest-objects">FormRequest objects</a></li>
        
            <li><a class="toctree-l4" href="#request_1">Request用法示例</a></li>
        
            <li><a class="toctree-l4" href="#response">Response对象</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#response_1">Response子类</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#textresponse">TextResponse对象</a></li>
        
            <li><a class="toctree-l4" href="#htmlresponse">HtmlResponse对象</a></li>
        
            <li><a class="toctree-l4" href="#xmlresponse">XmlResponse对象</a></li>
        
        </ul>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../15-extractors/">Link Extractors(链接提取器)</a>
                </li>
                <li class="">
                    
    <a class="" href="../16-settings/">Settings(设置)</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">3. 内置服务</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../3. 内置服务/17-logging/">Logging(日志记录)</a>
                </li>
                <li class="">
                    
    <a class="" href="../../3. 内置服务/18-stats-collection/">Stats Collection(状态收集)</a>
                </li>
                <li class="">
                    
    <a class="" href="../../3. 内置服务/19-send-email/">Sending e-mail(发送一个邮件)</a>
                </li>
                <li class="">
                    
    <a class="" href="../../3. 内置服务/20-telnet-console/">Telnet Console(Telnet控制台)</a>
                </li>
                <li class="">
                    
    <a class="" href="../../3. 内置服务/21-web-service/">Web Service(Web服务)</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">4. 解决具体问题</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../4. 解决具体问题/01-常见问题解答/">常见问题解答</a>
                </li>
                <li class="">
                    
    <a class="" href="../../4. 解决具体问题/02-debugind-spider/">Debugging Spiders(调试Spiders)</a>
                </li>
                <li class="">
                    
    <a class="" href="../../4. 解决具体问题/03-Spiders Contracts/">Spiders Contracts</a>
                </li>
                <li class="">
                    
    <a class="" href="../../4. 解决具体问题/04-common-practices/">Common Practices(常用实践)</a>
                </li>
                <li class="">
                    
    <a class="" href="../../4. 解决具体问题/05-broad-crawls/">Broad Crawls</a>
                </li>
                <li class="">
                    
    <a class="" href="../../4. 解决具体问题/06-use-firefox-scraping/">使用Firefox进行抓取(没写)</a>
                </li>
                <li class="">
                    
    <a class="" href="../../4. 解决具体问题/07-use-firbug-scraping/">使用Firebug进行抓取(没写)</a>
                </li>
                <li class="">
                    
    <a class="" href="../../4. 解决具体问题/08-debugging-memory/">调试内存泄漏</a>
                </li>
                <li class="">
                    
    <a class="" href="../../4. 解决具体问题/09-down-file-image/">下载并处理文件和图像</a>
                </li>
                <li class="">
                    
    <a class="" href="../../4. 解决具体问题/10-deploying-spider/">部署Spider</a>
                </li>
                <li class="">
                    
    <a class="" href="../../4. 解决具体问题/11-auto-throttle-extension/">AutoThrottle extension(自动节流扩展)</a>
                </li>
                <li class="">
                    
    <a class="" href="../../4. 解决具体问题/12-Benchmark/">基准测试</a>
                </li>
                <li class="">
                    
    <a class="" href="../../4. 解决具体问题/13-pausing-resuming-crawls/">作业: 暂停和恢复爬虫</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">5. Scrapy扩展</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../5. Scrapy扩展/01-Architecture-overview/">架构概述</a>
                </li>
                <li class="">
                    
    <a class="" href="../../5. Scrapy扩展/02-Downloader-Middleware/">Downloader Middleware</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Scrapy 1.5.1 中文文档</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>2. 基本概念 &raquo;</li>
        
      
    
    <li>Requests和Responses</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="requestsresponses">Requests和Responses</h1>
<p>Scrapy使用<a href="#">Request</a>和<a href="#">Response</a>对象来抓取web站点。</p>
<p>通常，<a href="#">Request</a>对象在spiders中生成并在系统中传递，直到它们到达Downloader，Downloader执行request并返回发出request的spider的<a href="#">Response</a>对象。</p>
<p><a href="#">Request</a>类和<a href="#">Response</a>类都有添加基类中不需要的功能的子类。下面在<a href="#">请求子类</a>和<a href="#">响应子类</a>中描述了这些。</p>
<h2 id="request">Request对象</h2>
<h4 id="scrapyhttprequesturl-callback-methodget-headers-body-cookies-meta-encodingutf-8-priority0-dont_filterfalse-errback-flags-class">scrapy.http.Request(url[, callback, method='GET', headers, body, cookies, meta, encoding='utf-8', priority=0, dont_filter=False, errback, flags])   这是一个类(class)</h4>
<blockquote>
<p><a href="#">Request</a>对象表示HTTP请求，通常在Spider中生成并由Downloader执行，从而生成<a href="#">Response</a>。</p>
<blockquote>
<p>参数:</p>
<blockquote>
<p><strong>url</strong> (字符串) – 此次请求的url</p>
<p><strong>callback</strong> (可调用的) — 这个函数将被调用，并将此request的response(下载后)作为其第一个参数。有关更多信息，请参阅<a href="#">将额外数据传递给回调函数</a>。如果请求没有指定callback，将使用spider的<a href="#">parse()</a>方法。注意，如果在处理期间引发异常，则会调用<code>errback</code>。</p>
<p><strong>method</strong> (字符串) – 此request的HTTP方法。默认为'GET'.</p>
<p><strong>meta</strong> (字典) – <a href="#">Request.meta</a>的初始值。元属性。如果给定，在此参数中传递的dict将被浅复制。</p>
<p><strong>body</strong> (字符串或unicode) – request主体。如果传递了<code>unicode</code>，则使用传递的编码(默认为utf-8)将其编码为str。如果没有给出body，则存储一个空字符串。不管这个参数的类型是什么，最终存储的值都将是<code>str</code>(从不使用<code>unicode</code>或<code>None</code>)。</p>
<p><strong>headers</strong> (字典) – 此请求的headers。字典的值可以是字符串(用于单值headers)或列表(用于多值headers)。如果<code>None</code>作为值传递，则根本不会发送HTTP头。</p>
<p><strong>cookies</strong> (字典或列表) –此请求的cookie。可以以两种形式发送。</p>
</blockquote>
</blockquote>
</blockquote>
<ul>
<li>使用字典</li>
</ul>
<pre><code class="python">request_with_cookies = Request(url=&quot;http://www.example.com&quot;,
                               cookies={'currency': 'USD', 'country': 'UY'})
</code></pre>

<ul>
<li>使用字典列表</li>
</ul>
<pre><code class="python">request_with_cookies = Request(url=&quot;http://www.example.com&quot;,
                               cookies=[{'name': 'currency',
                                        'value': 'USD',
                                        'domain': 'example.com',
                                        'path': '/currency'}])
</code></pre>

<blockquote>
<blockquote>
<blockquote>
<ul>
<li>
<p>后一种形式允许定制cookie的<code>domain</code>和<code>path</code>属性。这只在cookie为以后的请求保存时有用。</p>
</li>
<li>
<p>当某些站点返回cookie(在response中)时，这些cookie存储在该域的cookie中，并将在以后的request中再次发送。这是任何普通web浏览器的典型行为。但是，如果出于某种原因，您想避免与现有cookie合并，那么可以通过在<a href="#">Request.meta</a>中将<code>dont_merge_cookie</code>键设置为True来指示Scrapy这样做。</p>
</li>
</ul>
<p>request不合并cookie的例子:</p>
</blockquote>
</blockquote>
</blockquote>
<pre><code class="python">request_with_cookies = Request(url=&quot;http://www.example.com&quot;,
                               cookies={'currency': 'USD', 'country': 'UY'},
                               meta={'dont_merge_cookies': True})
</code></pre>

<blockquote>
<blockquote>
<blockquote>
<p>有关更多信息，请参阅<a href="#">CookiesMiddleware</a>。</p>
<p>encoding (字符串) – 此请求的编码(默认为<code>'utf-8'</code>)。 这种编码将用于对URL进行百分比编码，并将body转换为<code>str</code>(如果以<code>unicode</code>表示的话)。</p>
<p>priority (整型) – 此请求的优先级(默认为<code>0</code>).调度程序使用该优先级定义处理请求的顺序。具有更高优先级值的请求将更早执行。为了表示相对较低的优先级，允许使用负值。</p>
<p>dont_filter (布尔) - 指示调度程序不应该过滤此请求。当您想要多次执行相同的请求，以忽略去重过滤器时，可以使用这种方法。小心使用它，否则你会陷入爬行循环。默认为<code>False</code>。</p>
<p>errback(可调用的) - 在处理请求时，如果出现任何异常，将调用该函数。这包括404 HTTP错误等失败的页面。它接收一个<a href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Twisted Failure</a>实例作为第一个参数。有关更多信息，请参见下面<a href="#">使用errbacks捕获请求处理中的异常</a>。</p>
<p>flags(列表)-发送到请求的flag，可以用于日志记录或类似目的。</p>
</blockquote>
<p><code>url</code></p>
<blockquote>
<p>包含此请求URL的字符串。请记住，该属性包含转义URL，因此它可能与在构造函数中传递的URL不同。</p>
<p>这个属性是只读的。要更改请求的URL，请使用<a href="#">replace()</a>。</p>
</blockquote>
<p><code>method</code></p>
<blockquote>
<p>表示请求的HTTP方法的字符串。这保证是大写的。例如:<code>GET</code>, <code>POST</code>, <code>PUT</code>等等</p>
</blockquote>
<p><code>headers</code></p>
<blockquote>
<p>包含请求headers的类似字典的对象。</p>
</blockquote>
<p><code>body</code></p>
<blockquote>
<p>包含请求body的字符串。</p>
<p>这个属性是只读的。要更改请求的body，请使用<a href="#">replace()</a>。</p>
</blockquote>
<p><code>meta</code></p>
<blockquote>
<p>包含此请求的任意metadata的字典。这个dict对于新请求是空的，通常由不同的Scrapy组件(extensions(扩展)、middlewares(中间件)等)填充。因此，字典中包含的数据取决于您启用的(extensions(扩展)。</p>
<p>看一下<a href="#">Request.meta特殊键</a>。由Scrapy识别的一组特殊meta键。</p>
<p>当使用<a href="#">copy()</a>或<a href="#">replace()</a>方法克隆请求时，这个dict是<a href="https://docs.python.org/2/library/copy.html">浅复制</a>的，也可以在您的spider中从response访问<code>response.meta</code>属性。</p>
</blockquote>
<p><code>copy()</code></p>
<blockquote>
<p>返回一个新请求，该请求是此请求的副本。请参阅:<a href="#">将额外数据传递给回调函数</a>。</p>
</blockquote>
<p><code>replace([url, method, headers, body, cookies, meta, encoding, dont_filter, callback, errback])</code></p>
<blockquote>
<p>返回具有相同成员的请求对象，除了那些由指定的关键字参数赋予新值的成员。 默认的<a href="#">Request.meta</a>情况下，meta会被复制(除非在<code>meta</code>参数中给出了一个新的值)。请参阅<a href="#">向回调函数传递额外数据</a>。</p>
</blockquote>
</blockquote>
</blockquote>
<h2 id="callback">将额外数据传递给callback函数</h2>
<p>请求的callback函数是在下载该request的response时调用的函数。将使用下载的<a href="#">Response</a>对象作为其第一个参数调用callback函数。</p>
<p>例子:</p>
<pre><code class="python">def parse_page1(self, response):
    return scrapy.Request(&quot;http://www.example.com/some_page.html&quot;,
                          callback=self.parse_page2)

def parse_page2(self, response):
    # 这将会日志记录下来 http://www.example.com/some_page.html
    self.logger.info(&quot;Visited %s&quot;, response.url)
</code></pre>

<p>在某些情况下，您可能对向这些callback函数传递参数感兴趣，以便稍后在第二次callback中接收这些参数。您可以使用<a href="#">Request.meta</a>属性。</p>
<p>下面是如何使用这种机制传递item的示例，以填充来自不同页面的不同字段:</p>
<pre><code class="python">def parse_page1(self, response):
    item = MyItem()
    item['main_url'] = response.url
    request = scrapy.Request(&quot;http://www.example.com/some_page.html&quot;,
                             callback=self.parse_page2)
    request.meta['item'] = item
    yield request

def parse_page2(self, response):
    item = response.meta['item']
    item['other_url'] = response.url
    yield item
</code></pre>

<h2 id="errback">使用errback在请求处理中捕获异常</h2>
<p>请求的errback是在处理异常时调用的函数。</p>
<p>它接收一个<a href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Twisted Failure</a>实例作为第一个参数，可以用来跟踪连接建立超时、DNS错误等。</p>
<p>下面是一个spider记录所有错误的例子，如果需要，它可以捕获一些特定的错误:</p>
<pre><code class="python">import scrapy

from scrapy.spidermiddlewares.httperror import HttpError
from twisted.internet.error import DNSLookupError
from twisted.internet.error import TimeoutError, TCPTimedOutError

class ErrbackSpider(scrapy.Spider):
    name = &quot;errback_example&quot;
    start_urls = [
        &quot;http://www.httpbin.org/&quot;,              # 预计是HTTP 200
        &quot;http://www.httpbin.org/status/404&quot;,    # 没有找到的错误
        &quot;http://www.httpbin.org/status/500&quot;,    # 服务器问题
        &quot;http://www.httpbin.org:12345/&quot;,        # 未响应主机, 预计超时
        &quot;http://www.httphttpbinbin.org/&quot;,       # 预计是DNS错误
    ]

    def start_requests(self):
        for u in self.start_urls:
            yield scrapy.Request(u, callback=self.parse_httpbin,
                                    errback=self.errback_httpbin,
                                    dont_filter=True)

    def parse_httpbin(self, response):
        self.logger.info('Got successful response from {}'.format(response.url))
        # 做一些有用的事情...

    def errback_httpbin(self, failure):
        # 记录所有错误
        self.logger.error(repr(failure))

        # 如果你想对一些错误做一些特别的处理,
        # 你可能需要失败的类型:

        if failure.check(HttpError):
            # 这些异常来自HttpError spider 中间件
            # 你可以得到非200的response
            response = failure.value.response
            self.logger.error('HttpError on %s', response.url)

        elif failure.check(DNSLookupError):
            # 这是原始请求
            request = failure.request
            self.logger.error('DNSLookupError on %s', request.url)

        elif failure.check(TimeoutError, TCPTimedOutError):
            request = failure.request
            self.logger.error('TimeoutError on %s', request.url)
</code></pre>

<h2 id="requestmeta">Request.meta 特殊键</h2>
<p><a href="#">Request.meta</a>属性可以包含任意的数据，但是有一些特殊的键被Scrapy和它的内置扩展所识别。</p>
<p>这些是:</p>
<blockquote>
<ul>
<li><a href="#">dont_redirect</a></li>
<li><a href="#">dont_retry</a></li>
<li><a href="#">handle_httpstatus_list</a></li>
<li><a href="#">handle_httpstatus_all</a></li>
<li><a href="#">dont_merge_cookies</a></li>
<li><a href="#">cookiejar</a></li>
<li><a href="#">dont_cache</a></li>
<li><a href="#">redirect_urls</a></li>
<li><a href="#">bindaddress</a></li>
<li><a href="#">dont_obey_robotstxt</a></li>
<li><a href="#">download_timeout</a></li>
<li><a href="#">download_maxsize</a></li>
<li><a href="#">download_latency</a></li>
<li><a href="#">download_fail_on_dataloss</a></li>
<li><a href="#">proxy</a></li>
<li><code>ftp_user</code> （请参阅<a href="#">FTP_USER</a>以获得更多信息）</li>
<li><code>ftp_password</code>（请参阅<a href="#">FTP_PASSWORD</a>以获得更多信息）</li>
<li><a href="#">referrer_policy</a></li>
<li><a href="#">max_retry_times</a></li>
</ul>
</blockquote>
<h2 id="bindaddress">bindaddress</h2>
<p>输出IP地址的IP用于执行请求。</p>
<h2 id="download_timeout">download_timeout</h2>
<p>在超时之前，downloader将等待最大时间（秒）。参见:<a href="#">DOWNLOAD_TIMEOUT</a>。</p>
<h2 id="download_latency">download_latency</h2>
<p>用于获取response的时间，因为request已经启动，即通过网络发送的HTTP消息。只有当response被下载时，这个meta键才会出现。而大多数其他meta键都是用来控制Scrapy行为的，但是这个应该是只读的。</p>
<h2 id="download_fail_on_dataloss">download_fail_on_dataloss</h2>
<p>是否会在损坏的response中失败。看到:<a href="#">DOWNLOAD_FAIL_ON_DATALOSS</a>。</p>
<h2 id="max_retry_times">max_retry_times</h2>
<p>这个meta键被用来设置每个请求的重试时间。当初始化时，<a href="#">max_retry_times</a> meta键比<a href="#">RETRY_TIMES</a>设置优先级更高。</p>
<h1 id="request-subclasses">Request subclasses</h1>
<p>下面是内置<a href="#">Request</a>子类的列表。您还可以对它进行子类化来实现您自己的定制功能。</p>
<h2 id="formrequest-objects">FormRequest objects</h2>
<p>FormRequest类扩展了<a href="#">Request</a>基类，并提供了处理HTML表单的功能。它使用<a href="https://lxml.de/lxmlhtml.html#forms">lxml.html表单</a>从<a href="#">Response</a>对象的表单数据预先填充表单字段</p>
<h4 id="scrapyhttpformrequesturl-formdata-class">scrapy.http.FormRequest(url[, formdata, ...])   这是一个类(class)</h4>
<p><a href="#">FormRequest</a>类向构造函数添加了一个新的参数。剩下的参数与<a href="#">Request</a>类相同，这里没有文档记录。</p>
<blockquote>
<p>参数: </p>
<blockquote>
<p><strong>formdata</strong> (字典或者是一个可迭代的元组) - 是一个字典(或者是一个可迭代的键值对元组)包含HTML表单数据，这些数据将被url编码并分配给request的body。</p>
</blockquote>
</blockquote>
<p>除了标准<a href="#">Request</a>方法之外，<a href="#">FormRequest</a>对象还支持下列类方法：</p>
<blockquote>
<blockquote>
<p><strong>from_response</strong>(response[, formname=None, formid=None, formnumber=0, formdata=None, formxpath=None, formcss=None, clickdata=None, dont_click=False, ...])   这是一个类方法(classmethod)</p>
<blockquote>
<p>返回一个新的<a href="#">FormRequest</a>对象，它的表单字段值预先填充了在给定response中包含的HTML<code>&lt;form&gt;</code>元素中的元素。举个例子，看一下<a href="#">使用FormRequest.from_response()来模拟用户登录</a>。</p>
<p>该策略是在默认情况下，在任何看起来可点击的元素上自动模拟点击，就像<code>&lt;input type="submit"&gt;</code>。尽管这很方便，而且通常是我们想要的行为，但有时它会导致难以调试的问题。例如，在使用javascript填写和/或提交的表单时，默认的<code>from_response()</code>行为可能不是最合适的。要禁用这种行为，您可以将<code>dont_click</code>参数设置为<code>True</code>。另外，如果你想要改变控制点击（而不是禁用它），你也可以使用<code>clickdata</code>参数</p>
</blockquote>
</blockquote>
</blockquote>
<pre><code class="warning">警告!!!!!!!
使用这种方法，选择具有前导的元素或在选项值中结尾空白将无法工作，
因为lxml中的一个bug(https://bugs.launchpad.net/lxml/+bug/1665241)，
应该在lxml 3.8和更高版本中修复。
</code></pre>

<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>参数:</p>
<blockquote>
<ul>
<li>
<p>response([Request]对象) - 包含一个HTML表单的response，该表单将用于预填充表单字段</p>
</li>
<li>
<p>formname(字符串) - 如果给定的话，带有name属性的表单将被使用。</p>
</li>
<li>
<p>formid (字符串) - 如果给定，带有id属性的表单将被使用</p>
</li>
<li>
<p>formxpath(字符串) - 如果给定, 第一个与xpath匹配的表单将被使用</p>
</li>
<li>
<p>formcss (字符串) – 如果给定, 与css选择器匹配的第一个表单将被使用。</p>
</li>
<li>
<p>formnumber (int整型) – 当响应包含多个表单时，要使用的表单数值地址。第一个（也是默认值）是<code>0</code></p>
</li>
<li>
<p>formdata (字典) – 在表单数据中要覆盖的字段。如果一个字段已经出现在response的<code>&lt;form&gt;</code>元素中，那么它的值就会被这个参数中传递的值所覆盖。如果在这个参数中传递的值是<code>None</code>，该字段将不包含在request中，即使它是在response的<code>&lt;form&gt;</code>元素中出现的。</p>
</li>
<li>
<p>clickdata (字典) – 查找可单击控件的属性。如果没有给出，表单数据将被提交模拟第一个可点击元素的点击。除了html属性之外，通过<code>nr</code>属性，相对于表单内的其他提交表输入, 可以通过其基于零的索引来识别控制。</p>
</li>
<li>
<p>dont_click(布尔类型) - 如果是<code>True</code>，表单数据将在不单击任何元素的情况下提交。</p>
</li>
</ul>
</blockquote>
</blockquote>
<p>这个类方法的其他参数直接传递给<a href="#">FormRequest</a>构造函数。</p>
<blockquote>
<p>在0.10.3新版本: 添加了<code>formname</code>参数。</p>
<p>在0.17新版本: 添加了<code>formxpath</code>参数。</p>
<p>在1.1.0:新版本: 添加了<code>formcss</code>参数。</p>
<p>在1.1.0新版本: 添加了<code>formid</code>参数。</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<h2 id="request_1">Request用法示例</h2>
<h3 id="formrequesthttp-post">使用FormRequest通过HTTP POST发送数据</h3>
<p>如果你想在你的spider中模拟一个HTML表单，并发送几个键值字段，你可以返回一个<a href="#">FormRequest</a>对象（来自你的spider），如下所示：</p>
<pre><code class="python">return [FormRequest(url=&quot;http://www.example.com/post/action&quot;,
                    formdata={'name': 'John Doe', 'age': '27'},
                    callback=self.after_post)]
</code></pre>

<h3 id="formrequestfrom_response">使用FormRequest.from_response()来模拟用户登录</h3>
<p>web站点通常通过<code>&lt;input type="hidden"&gt;</code>元素来提供预先填充的表单字段, 例如session相关的数据或身份验证token(用于登录页面), 在抓取时，您会希望这些字段是自动预填充的，并且您只想覆盖其中的几个字段，比如用户名和密码。您可以使用<a href="#">FormRequest.from_response()</a>方法来完成这项工作。这是一个使用它的spider：</p>
<pre><code class="python">import scrapy

class LoginSpider(scrapy.Spider):
    name = 'example.com'
    start_urls = ['http://www.example.com/users/login.php']

    def parse(self, response):
        return scrapy.FormRequest.from_response(
            response,
            formdata={'username': 'john', 'password': 'secret'},
            callback=self.after_login
        )

    def after_login(self, response):
        # 在继续之前，检查是否登录成功
        if &quot;authentication failed&quot; in response.body:
            self.logger.error(&quot;Login failed&quot;)
            return

        # 继续用经过身份验证的会话进行抓取...
</code></pre>

<hr />
<h2 id="response">Response对象</h2>
<h4 id="scrapyhttpresponseurl-status200-headersnone-bodyb-flagsnone-requestnone-class">scrapy.http.Response(url[, status=200, headers=None, body=b'', flags=None, request=None])   这是一个类(class)</h4>
<blockquote>
<p><a href="#">Response</a>对象代表HTTP response, 它通常被下载（由Downloader），并喂给Spiders进行处理。</p>
<blockquote>
<p>参数:</p>
<blockquote>
<ul>
<li>
<p>url(字符串) – 请求url的response</p>
</li>
<li>
<p>status (int整型) – response的HTTP状态。默认为200。</p>
</li>
<li>
<p>headers(字典) – 这个response的headers。可以是字符串（对于单值头）或列表（对于多值头）。</p>
</li>
<li>
<p>body (bytes) – response的body。将解码的文本作为str(Python 2中的unicode)，您可以使用<code>response.text</code>来自encoding-aware <a href="#">Request</a>子类的文本，如<a href="#">TextResponse</a>。</p>
</li>
<li>
<p>flags (列表) – 是一个包含<a href="#">Response.flags</a>属性的初始值的列表。 如果给定，该列表将被简单地复制。</p>
</li>
<li>
<p>request (<a href="#">Request</a>对象) – <a href="#">Response.request</a>属性的初始值。 这表示产生此response的<a href="#">Request</a>。</p>
</li>
</ul>
</blockquote>
</blockquote>
<p><code>url</code></p>
<blockquote>
<p>包含response的URL的字符串。</p>
<p>这个属性是只读的。要改变response的URL使用<a href="#">replace()</a></p>
</blockquote>
<p><code>status</code></p>
<blockquote>
<p>一个表示response的HTTP状态的整数。 例子: <code>200</code>, <code>404</code></p>
</blockquote>
<p><code>headers</code></p>
<blockquote>
<p>一个类似于字典的对象，它包含response headers。可以使用<code>get()</code>来取值，用指定的名字返回第一个header的值或<code>getlist()</code>用指定的名字返回所有的header值。 例如，调用这个会提供headers中所有的cookie：</p>
<p><code>response.headers.getlist('Set-Cookie')</code></p>
</blockquote>
<p><code>body</code></p>
<blockquote>
<p>response的body。请牢记<code>Response.body</code>总是一个<code>bytes</code>对象。如果你想要unicode版本，使用<a href="#">TextResponse.text</a>(仅在<a href="#">TextResponse</a>和子类中可用)。</p>
<p>这个属性是只读的。要更改response body，请使用<a href="#">replace()</a>。</p>
</blockquote>
<p><code>request</code></p>
<blockquote>
<p>生成此response的<a href="#">Request</a>对象。在response和request通过所有<a href="#">Downloader Middlewares</a>之后，这个属性被分配到Scrapy引擎中。特别地，这意味着:</p>
<blockquote>
<ul>
<li>HTTP重定向将导致原始请求(重定向前的URL)被分配给重定向response(重定向后的最终URL)。</li>
<li>Response.request.url并不总是等于Response.url</li>
<li>该属性仅在spider代码和<a href="#">Spider Middlewares</a>中可用，但在<a href="#">Downloader Middlewares</a>中不可用(尽管您可以通过其他方式获得请求)和<a href="#">response_downloaded</a> 信号的处理程序中不可用。</li>
</ul>
</blockquote>
</blockquote>
<p><code>meta</code></p>
<blockquote>
<p><a href="#">Response.request</a>对象(即。<code>self.request.meta</code>)的<a href="#">Request.meta</a>属性的快捷方式。</p>
<p>不同于<a href="#">Response.request</a>属性, <a href="#">Response.meta</a>属性通过重定向和重试传播, 你会从你的spider得到原始的<a href="#">Request.meta</a>
。</p>
</blockquote>
</blockquote>
<pre><code class="note">参见Request.meta属性
</code></pre>

<blockquote>
<p><code>flags</code></p>
<blockquote>
<p>包含此response flag的列表。 flag是用于标记response的标签。例如:'cached'，'redirect'等等，它们显示在response的字符串表示上(__str__方法)，该方法被引擎用于日志记录。</p>
</blockquote>
<p><code>copy()</code></p>
<blockquote>
<p>返回一个新response，该response是此response的副本。</p>
</blockquote>
<p><code>replace([url, status, headers, body, request, flags, cls])</code></p>
<blockquote>
<p>返回具有相同成员的response对象，除了那些由指定的关键字参数赋予新值的成员。默认情况下，<a href="#">Response.meta</a>会被复制。</p>
</blockquote>
<p><code>urljoin(url)</code></p>
<blockquote>
<p>通过将response的url与相对url相结合来构造一个绝对url。</p>
<p>这是<a href="#">urlparse.urljoin</a>之上的一个包装器，它只是进行此调用的别名:
<code>urlparse.urljoin(response.url, url)</code></p>
</blockquote>
<p><strong>follow</strong>(url, callback=None, method='GET', headers=None, body=None, cookies=None, meta=None, encoding='utf-8', priority=0, dont_filter=False, errback=None)</p>
<blockquote>
<p>返回<a href="#">Request</a>实例以跟随链接<code>url</code>。它接受与<code>Request.__init__</code>方法相同的参数。，但<code>url</code>可以是相对url或<code>scrapy.link.Link</code>对象，不只是绝对URL。</p>
<p><a href="#">TextResponse</a>提供了<a href="#">follow()</a>方法，该方法除了支持绝对/相对url和链接对象之外，还支持选择器。</p>
</blockquote>
</blockquote>
<h1 id="response_1">Response子类</h1>
<h2 id="textresponse">TextResponse对象</h2>
<h4 id="scrapyhttptextresponseurl-encoding-class">scrapy.http.TextResponse(url[, encoding[, ...]])   这是一个类(class)</h4>
<p><a href="#">TextResponse</a>对象向<a href="#">Response</a>基类添加编码功能，<a href="#">Response</a>基类只用于二进制数据，如图像、声音或任何媒体文件。</p>
<p>除了基本<a href="#">Response</a>对象之外，<a href="#">TextResponse</a>对象还支持一个新的构造函数参数, 其余的功能与<a href="#">Response</a>类相同</p>
<blockquote>
<p>参数:</p>
<blockquote>
<ul>
<li>encoding (字符串) – 包含用于此response的编码的字符串, 如果您使用unicode body创建一个<a href="#">TextResponse</a>对象, 它将使用这种编码进行编码(请记住body属性总是一个字符串), 如果<code>encoding</code>为<code>None</code>(默认值)，则在response headers和body中查找编码。</li>
</ul>
</blockquote>
</blockquote>
<p>除了支持标准的<a href="#">Response</a>属性外，<a href="#">TextResponse</a>对象还支持以下属性:</p>
<p><code>text</code></p>
<blockquote>
<p>response body，作为unicode。</p>
<blockquote>
<p>与<code>response.body.decode(response.encoding)</code>相同，但是结果在第一次调用之后缓存，所以你可以访问<code>response.text</code>多次，没有额外的开销。</p>
</blockquote>
</blockquote>
<pre><code class="note">unicode(response.body)不是将response body转换为unicode的正确方法:
您将使用系统默认编码(通常是ascii)，而不是response编码。
</code></pre>

<p><code>encoding</code>
带有此response编码的字符串。按照顺序，可以通过尝试以下机制来解决编码问题:</p>
<blockquote>
<ol>
<li>在构造函数编码参数中传递的编码</li>
<li>HTTP headers头里Content-Type声明编码。如果此编码无效(即未知)，它被忽略，并尝试下一个解决机制。</li>
<li>response body体中声明的编码。<a href="#">TextResponse</a>类没有为此提供任何特殊功能。然而，<a href="#">HtmlResponse</a>和<a href="#">XmlResponse</a>类do。</li>
<li>编码是通过观察response body体来推断的。这是比较脆弱的方法，也是最后一个尝试的方法。</li>
</ol>
</blockquote>
<p><code>selector</code></p>
<blockquote>
<p>使用response作为目标的<a href="#">Selector</a>实例。选择器在第一次访问时惰性实例化。</p>
</blockquote>
<p><a href="#">TextResponse</a>对象除了支持标准<a href="#">Response</a>方法外，还支持以下方法:</p>
<p><code>xpath(query)</code></p>
<blockquote>
<p><code>TextResponse.selector.xpath(query)</code>的快捷方式</p>
<p><code>response.xpath('//p')</code></p>
</blockquote>
<p><code>css(query)</code></p>
<blockquote>
<p><code>TextResponse.selector.css(query)</code>的快捷方式</p>
<p><code>response.css('p')</code></p>
</blockquote>
<p><code>follow(url, callback=None, method='GET', headers=None, body=None, cookies=None, meta=None, encoding=None, priority=0, dont_filter=False, errback=None)</code></p>
<blockquote>
<p>返回<a href="#">Request</a>实例以跟随链接<code>url</code>。它接受与<code>Request.__init__</code>相同的参数。 但<code>url</code>不仅可以是一个绝对url，而且也是</p>
<blockquote>
<ul>
<li>一个相对URL</li>
<li>一个scrapy.link.Link对象(如link extractor(链接提取器)结果);</li>
<li>一个属性Selector(不是SelectorList) - 比如<code>response.css('a::attr(href)')[0]</code>或<code>response.xpath('//img/@src')[0]</code>。</li>
<li>一个<code>&lt;a&gt;</code>或<code>&lt;link&gt;</code>元素的Selector, 比如<code>response.css('a.my_link')[0]</code></li>
</ul>
</blockquote>
</blockquote>
<p>请参阅<a href="#">创建Requests的快捷方式</a>使用例子</p>
<p><code>body_as_unicode()</code></p>
<blockquote>
<p>与<a href="#">text</a>相同，但作为方法可用。这种方法是为了向后兼容;请选择<code>response.text</code>。</p>
</blockquote>
<h2 id="htmlresponse">HtmlResponse对象</h2>
<h4 id="scrapyhttphtmlresponseurl-class">scrapy.http.HtmlResponse(url[, ...])   这是一个类(class)</h4>
<blockquote>
<p><a href="#">HtmlResponse</a>类是<a href="#">TextResponse</a>的一个子类，它通过查找HTML的<a href="https://www.w3schools.com/TAGS/att_meta_http_equiv.asp">meta http-equiv</a>属性来添加编码自动发现支持。看<a href="#">TextResponse.encoding</a>。</p>
</blockquote>
<h2 id="xmlresponse">XmlResponse对象</h2>
<h4 id="scrapyhttpxmlresponseurl-class">scrapy.http.XmlResponse(url[, ...])   这是一个类(class)</h4>
<blockquote>
<p><a href="#">XmlResponse</a>类是<a href="#">TextResponse</a>的子类，它通过查看XML声明行添加编码自动发现支持。看<a href="#">TextResponse.encoding</a>。</p>
</blockquote>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../15-extractors/" class="btn btn-neutral float-right" title="Link Extractors(链接提取器)">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../13-feed_exports/" class="btn btn-neutral" title="Feed exports(导出各种格式文件)"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../13-feed_exports/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../15-extractors/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>

</body>
</html>
